\documentclass[11pt,a4j,dvipdfmx]{jsarticle}
  \input{settings.tex}
  \usepackage{style}
%
  \title{機械学習}
  \author{Takashi SUWA}
\begin{document}
  \maketitle
%
  \section{最小 \(2\) 乗回帰}
    \subsection{線型モデル}
    \indent
      あらかじめ判明している有限個の入力・出力の組の例
      \(\Data \defeq \setprnindex{\seqprn{\vecx_i, y_i}}{\setR^{d} \times \setR}{i}{\Natleq{n}}\)
      をもとに，各 \(i \in \Natleq{n}\) に対して \(\app{f}{\vecx_i} \approx y_i\) を満たし，
      かつ \(\Data\) に含まれていない入力に対してもうまく汎化された函数
      \(\funcdoms{f}{\setR^{d}}{\setR}\) を“学習”することを考える．
      この学習は\newword{回帰}と呼ばれ，回帰の元とする集合 \(\Data\) を
      \newword{訓練標本}或いは\newword{教師データ}などと呼ぶ．
      勿論 \(f\) が何の制約もない函数であれば訓練標本以外の入力に対しては全く判ることがないから，
      \(f\) はをあらかじめ決めた基底函数族
      \(\BaseFunc \defeq \setprnindex{\basefunc_j}{\doms{\setR^{d}}{\setR}}{j}{\Natleq{b}}\)
      の線型結合で書けるものと設定する．すなわち，求める函数が
      \(\vectheta \defeq \trsps{\seqprn{\theta_1, \ldots, \theta_b}}\)
      をパラメータとして
      \(\DS \app{f_{\vectheta}}{\vecx} \defeq
        \sum_{\scriptrange{j \in \Natleq{b}}} \theta_j \app{\basefunc_j}{\vecx}\)
      の形であることを仮定して，訓練標本 \(\Data\) をもとに \(\vectheta\) を“特定”する．
      この設定を\newword{線型モデル}と呼ぶ．
    \par
    \subsection{カーネルモデル}
    \indent
      線型モデルのうち，あらかじめ決めた函数 \(\funcdoms{K}{\setR^{d} \times \setR^{d}}{\setR}\) を使い，
      訓練標本
      \(\Data \defeq \setprnindex{\seqprn{\vecx_i, y_i}}{}{i}{\Natleq{n}}\)
      に依存して
      基底函数族
      \(\BaseFunc \defeq \setprnindex{\basefunc_j}{}{j}{\Natleq{b}}\)
      を \(b \defeq n\)，\(\basefunc_j \defeq \app{K}{\vecx_j, \placeholder}\) と
      定めるものを\newword{カーネルモデル}と呼ぶ．
      基底函数族の大きさ \(b\) が訓練標本の大きさに依存することが特徴である．
%      線型モデルの特殊な場合として\newword{ノンパラメトリックモデル}とも呼ばれる．
    \par
    \example{Gau\ss カーネル}{
      \(\DS \app{K}{\vecc, \vecx} \defeq
        \exp \paren{-\slashfrac{\distprn{\vecx - \vecc}^{2}}{2 h^{2}}}\)
      としたカーネルモデルである．
      要するに \(f_{\vectheta}\) は未知の入力に対して
      訓練入力標本からの距離を“重率”として使ってパラメータを足し合わせて出力する．
      これは訓練入力標本が \(\setR^{d}\) 内で偏って分布しているときに
      訓練入力標本が分布していない領域を実質的に無視することに相当し，次元削減効果が期待できる．
    }
    \subsection{最小 \(2\) 乗回帰}
    \indent
      ここまでの設定に基づくと，基底函数族
      \(\BaseFunc \defeq \setprnindex{\basefunc_j}{}{j}{\Natleq{b}}\)
      による学習モデル \(f_{\vectheta}\) と訓練標本
      \(\Data \defeq \setprnindex{\seqprn{\vecx_i, y_i}}{}{i}{\Natleq{n}}\)
      からパラメータ \(\vectheta\) を特定することが回帰の根幹となる．
      \newword{最小 \(2\)~乗回帰}は，\(2\)~乗誤差
      \(\DS \app{\ErrorLS}{\vectheta} \defeq \frac{1}{2} \sum_{\scriptrange{i \in \Natleq{n}}}
        \paren{\app{f_{\vectheta}}{\vecx_i} - y_i}^{2}\)
      を最小にするような \(\vectheta\) を求める方法である．
      \begin{align*}
        \mPhi &\defeq
          \begin{pmatrix}
            \app{\basefunc_1}{\vecx_1} & \cdots & \app{\basefunc_b}{\vecx_1}
          \\
            \vdots & \ddots & \vdots
          \\
            \app{\basefunc_1}{\vecx_n} & \cdots & \app{\basefunc_b}{\vecx_n}
          \end{pmatrix}
      \end{align*}
      で定められる行列を\newword{計画行列}と呼び，これを用いると
      \begin{align*}
        \app{f_{\vectheta}}{\vecx_i}
        &=
          \sum_{\scriptrange{j \in \Natleq{b}}} \theta_j \app{\basefunc_j}{\vecx_i}
        =
          \sum_{\scriptrange{j \in \Natleq{b}}} \accessm{\mPhi}{i}{j} \accessv{\vectheta}{j}
        =
          \accessv{\paren{\mPhi \vectheta}}{i}
      \end{align*}
      と表せるから，\(\vecy \defeq \trsps{\seqprn{y_1, \ldots, y_n}}\) とおくと
      \begin{align*}
        \app{\ErrorLS}{\vectheta}
        &=
          \frac{1}{2} \sum_{\scriptrange{i \in \Natleq{n}}} \paren{\app{f_{\vectheta}}{\vecx_i} - y_i}^{2}
      \\&=
          \frac{1}{2} \sum_{\scriptrange{i \in \Natleq{n}}} \paren{\accessv{\paren{\mPhi \vectheta}}{i} - \accessv{\vecy}{i}}^{2}
      \\&=
          \frac{1}{2} \distprn{\mPhi \vectheta - \vecy}^{2}
        =
          \frac{1}{2} \trsps{\paren{\mPhi \vectheta - \vecy}} \paren{\mPhi \vectheta - \vecy}
        =
          \frac{1}{2} \trsps{\vectheta} \trsps{\mPhi} \mPhi \vectheta - \trsps{\vecy} \mPhi \vectheta + \frac{1}{2} \trsps{\vecy} \vecy
      \end{align*}
      となる．したがって最小 \(2\)~乗回帰を与える \(\vectheta\) の最適解 \(\vecthetaLS\) は
      \begin{align*}
        0 = \app{\paren{\nabla \ErrorLS}}{\vectheta}
        &\iff
          0 = \trsps{\mPhi} \mPhi \vectheta - \trsps{\mPhi} \vecy
      \\&\iff
          \trsps{\mPhi} \mPhi \vectheta = \trsps{\mPhi} \vecy
      \end{align*}
      より \(\trsps{\mPhi} \mPhi\) の正則性を仮定すると \(\vecthetaLS = \paren{\trsps{\mPhi} \mPhi}^{-1} \trsps{\mPhi} \vecy\)
      である．
    \par
    \indent
      ただし，この最小 \(2\)~乗回帰をそのまま適用すると訓練標本に特化して複雑化しすぎた函数が得られがちである．この現象を\newword{過学習}と呼ぶ．
      一般には訓練標本そのものにもノイズが乗っているはずなので，訓練標本の多少の変化では得られる函数が大きく変わらず，頑強であることが望ましい．
      これを実現する方法として \(\vectheta\) に何らかの制限をかける\newword{正則化}という方法をとる．
    \par
    \indent
      シンプルな方法として，何らかの“半径”のパラメータ \(r \in \setRpos\) を適切に定めて \(\distprn{\vectheta} \leq r\) なる制約をかけるというものがある．
      すなわち最適化問題としては
      \Minimize{%
        \app{\ErrorLS}{\vectheta}
      }{%
        \distprn{\vectheta} &\leq r
      }{}
      と表される．ただし，この制約は“厳密に守らないといけないもの”ではないから，ペナルティ函数に緩和して制約なし最適化問題へと変形する：
      \Minimize{%
        \app{\ErrorLS}{\vectheta} + \frac{1}{2} \lambda \distprn{\vectheta}^{2}
      }{}{}
      ペナルティ函数につくパラメータ \(\lambda \in \setRpos\) は“元の \(r\) に応じて決まる”ものであるが，実際には \(\lambda\) を直接指定する．
      この手法を\newword{\(\ell^{2}\)-正則化}と呼ぶ．
      \(\ell^{2}\)-正則化を施した最適化問題も，もともとの最小 \(2\)~乗回帰と同様に
      \begin{align*}
        \app{\ErrorLS}{\vectheta} + \frac{1}{2} \lambda \distprn{\vectheta}^{2}
        &=
          \paren{\frac{1}{2} \trsps{\vectheta} \trsps{\mPhi} \mPhi \vectheta
            - \trsps{\vecy} \mPhi \vectheta + \frac{1}{2} \trsps{\vecy} \vecy}
            + \frac{1}{2} \lambda \trsps{\vectheta} \mI \vectheta
      \\&=
          \frac{1}{2} \trsps{\vectheta} \paren{\trsps{\mPhi} \mPhi + \lambda \mI} \vectheta
            - \trsps{\vecy} \mPhi \vectheta + \frac{1}{2} \trsps{\vecy} \vecy
      \end{align*}
      と変形できるから，\(\trsps{\mPhi} \mPhi + \lambda \mI\) の正則性を仮定すると
      \begin{align*}
        0 = \app{\nabla \ErrorLS}{\vectheta}
        &\iff
          0 = \paren{\trsps{\mPhi} \mPhi + \lambda \mI} \vectheta - \trsps{\mPhi} \vecy
      \\&\iff
          \paren{\trsps{\mPhi} \mPhi + \lambda \mI} \vectheta = \trsps{\mPhi} \vecy
      \\&\iff
          \vectheta = \paren{\trsps{\mPhi} \mPhi + \lambda \mI}^{-1} \trsps{\mPhi} \vecy
      \end{align*}
      と解くことができる．
    \par
\end{document}
